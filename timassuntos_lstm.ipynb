{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trabalhista_lstm.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ricardordb/deep/blob/master/timassuntos_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDfuJSvOIQ6h",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Files\n",
        "Please upload train data and Word2Vec model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89xpx4RHyXh0",
        "colab_type": "code",
        "outputId": "6834400b-df13-4707-a398-885270463025",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c52dd46e-7935-41b3-8697-bce5ce38a428\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c52dd46e-7935-41b3-8697-bce5ce38a428\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r387i-J1LFNF",
        "colab_type": "code",
        "outputId": "52484f39-e742-4a9e-a01a-f346253acadd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWOIjjPqazrk",
        "colab_type": "text"
      },
      "source": [
        "# Classify Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6Nsc-OIaxTX",
        "colab_type": "code",
        "outputId": "9cb4ac3f-4f51-43c7-eb1c-876623d627a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, Concatenate, Activation\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam, Adamax, Nadam\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "\n",
        "import keras.backend as K\n",
        "if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
        "  from keras.layers import CuDNNLSTM as LSTM\n",
        "  from keras.layers import CuDNNGRU as GRU\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from unicodedata import normalize as normtext\n",
        "from string import punctuation\n",
        "from joblib import dump\n",
        "\n",
        "\n",
        "datapath = '/content/gdrive/My Drive/Colab Notebooks/data/'\n",
        "\n",
        "trantab_all = str.maketrans(punctuation, len(punctuation)*\" \")\n",
        "\n",
        "cachedStopWords = list(map(lambda x : normtext('NFKD', x).encode('ASCII', 'ignore').decode('ASCII').lower(),\n",
        "                           stopwords.words(\"portuguese\")))\n",
        "\n",
        "\n",
        "def pp_texto_resultado(text):\n",
        "    if text is not None:\n",
        "        # minusculos e tira acentuacao\n",
        "        text = normtext('NFKD', text).encode('ASCII', 'ignore').decode('ASCII').lower()\n",
        "        text = text.translate(trantab_all) #tira pontuacao\n",
        "        text = re.sub(r'\\d{5,}', '', str(text).strip()) # Retira todo número maior que 5 algarismos\n",
        "        text = remove_stop(text) # retira stopwords\n",
        "    else:\n",
        "        return ''\n",
        "    return text\n",
        "\n",
        "def remove_stop(text):\n",
        "    return ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
        "\n",
        "\n",
        "def preprocess(texts, tokenizer, maxlen=500):\n",
        "  texts = map(pp_texto_resultado, texts)\n",
        "  texts = tokenizer.texts_to_sequences(texts)\n",
        "  return pad_sequences(texts, maxlen=maxlen)\n",
        "  \n",
        "\n",
        "def create_embedding_layer():\n",
        "  # load pre-trained word embeddings into an Embedding layer\n",
        "  # note that we set trainable = False so as to keep the embeddings fixed\n",
        "  return Embedding(\n",
        "            MAX_VOCAB_SIZE,\n",
        "            EMBEDDING_DIM,\n",
        "            weights=[embedding_matrix],\n",
        "            input_length=MAX_SEQUENCE_LENGTH,\n",
        "            trainable=False\n",
        "          )\n",
        "\n",
        "def rnn():\n",
        "    embedding_layer = create_embedding_layer()\n",
        "    input_1 = Input(shape=(MAX_SEQUENCE_LENGTH,), name='Texto')\n",
        "    input_2 = Input(shape=(MAX_SEQUENCE_LENGTH,), name='Assunto')\n",
        "    x1 = embedding_layer(input_1)\n",
        "    x2 = embedding_layer(input_2)\n",
        "    x = Concatenate()([x1, x2])\n",
        "    #x = GRU(128, return_sequences=True)(x)\n",
        "    #x = LSTM(128, return_sequences=True)(x)\n",
        "#     x = Dropout(0.2)(x)    \n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "#     x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "#     x = Dropout(0.2)(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    output = Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = Model([input_1, input_2], output)\n",
        "    model.compile(\n",
        "            loss='binary_crossentropy',\n",
        "            optimizer=Nadam(lr=0.01),\n",
        "            metrics=['accuracy']\n",
        "          )\n",
        "    return model\n",
        "  \n",
        "\n",
        "# some configuration\n",
        "VALIDATION_SPLIT = 0.2\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 40\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "\n",
        "\n",
        "word_vectors = Word2Vec.load(datapath + \"w2v_trabalhista.model\")\n",
        "EMBEDDING_DIM = word_vectors.vector_size\n",
        "MAX_VOCAB_SIZE = len(word_vectors.wv.index2word) + 1\n",
        "print('Found %s by %s word vectors.' % (MAX_VOCAB_SIZE, EMBEDDING_DIM))\n",
        "\n",
        "\n",
        "# prepare text samples and their labels\n",
        "print('Loading in comments...')\n",
        "\n",
        "train = pd.read_excel(datapath + \"train_sentiment.xlsx\", usecols=['texto', 'assunto', 'classify']).dropna()\n",
        "texto = train['texto'].apply(pp_texto_resultado)\n",
        "assunto = train['assunto'].apply(pp_texto_resultado)\n",
        "#Loss: binary_crossentropy Activation: sigmoid\n",
        "targets = train[\"classify\"]\n",
        "#Loss: hinge   Activation: tanh\n",
        "#targets = train[\"Sentiment\"].values \n",
        "\n",
        "# convert the sentences (strings) into integers\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(texto)\n",
        "texto_seq = tokenizer.texts_to_sequences(texto)\n",
        "assunto_seq = tokenizer.texts_to_sequences(assunto)\n",
        "\n",
        "\n",
        "# get word -> integer mapping\n",
        "word2idx = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word2idx))\n",
        "\n",
        "\n",
        "# pad sequences so that we get a N x T matrix\n",
        "texto_data = pad_sequences(texto_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "assunto_data = pad_sequences(assunto_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensors:', (texto_data.shape, assunto_data.shape))\n",
        "\n",
        "\n",
        "\n",
        "# prepare embedding matrix\n",
        "print('Filling pre-trained embeddings...')\n",
        "num_words = MAX_VOCAB_SIZE\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word2idx.items():\n",
        "  if i < MAX_VOCAB_SIZE:\n",
        "       # words not found in embedding index will be all zeros.\n",
        "      try:\n",
        "          embedding_vector = word_vectors.wv.get_vector(word)\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "      except:\n",
        "          pass\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0805 15:19:26.614912 140390381836160 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0805 15:19:26.616068 140390381836160 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0805 15:19:26.616883 140390381836160 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0805 15:19:27.897845 140390381836160 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 11204 by 100 word vectors.\n",
            "Loading in comments...\n",
            "Found 95260 unique tokens.\n",
            "Shape of data tensors: ((100814, 500), (100814, 500))\n",
            "Filling pre-trained embeddings...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5cFjA9q_GMt",
        "colab_type": "code",
        "outputId": "61dbae21-739b-49d0-de26-c9e6c4d5b355",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "print('Building model...')\n",
        "\n",
        "model = rnn()\n",
        "\n",
        "model.summary()\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=8, verbose=2, restore_best_weights=True)\n",
        "rop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=2)\n",
        "\n",
        "print('Training model...')\n",
        "r = model.fit(\n",
        "  [texto_data, assunto_data],\n",
        "  targets,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  epochs=EPOCHS,\n",
        "  validation_split=VALIDATION_SPLIT,\n",
        "  callbacks=[es, rop]\n",
        ")\n",
        "\n",
        "# plot some data\n",
        "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# accuracies\n",
        "plt.plot(r.history['acc'], label='acc')\n",
        "plt.plot(r.history['val_acc'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "p = model.predict([texto_data, assunto_data])\n",
        "print(roc_auc_score(targets, p))\n",
        "pc = np.round(p)\n",
        "print(confusion_matrix(targets, pc))\n",
        "\n",
        "#train['predictions'] = np.where(pc==0, -1, pc) \n",
        "#train['probs'] = p\n",
        "\n",
        "model.save_weights(datapath + 'classify_trab_weights.h5')\n",
        "dump(embedding_matrix, datapath + 'embedding_matrix.joblib.gz', compress='gzip')\n",
        "dump(tokenizer, datapath + 'tokenizer.joblib.gz', compress='gzip')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0805 15:20:52.165421 140390381836160 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0805 15:20:55.796332 140390381836160 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0805 15:20:55.807522 140390381836160 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Texto (InputLayer)              (None, 500)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Assunto (InputLayer)            (None, 500)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 500, 100)     1120400     Texto[0][0]                      \n",
            "                                                                 Assunto[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 500, 200)     0           embedding_1[0][0]                \n",
            "                                                                 embedding_1[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 500, 256)     337920      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_1 (GlobalM (None, 256)          0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            257         global_max_pooling1d_1[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 1,458,577\n",
            "Trainable params: 338,177\n",
            "Non-trainable params: 1,120,400\n",
            "__________________________________________________________________________________________________\n",
            "Training model...\n",
            "Train on 80651 samples, validate on 20163 samples\n",
            "Epoch 1/40\n",
            "80651/80651 [==============================] - 67s 825us/step - loss: 0.1359 - acc: 0.9478 - val_loss: 0.1015 - val_acc: 0.9632\n",
            "Epoch 2/40\n",
            "80651/80651 [==============================] - 64s 790us/step - loss: 0.0799 - acc: 0.9711 - val_loss: 0.0891 - val_acc: 0.9686\n",
            "Epoch 3/40\n",
            "80651/80651 [==============================] - 64s 791us/step - loss: 0.0680 - acc: 0.9761 - val_loss: 0.2100 - val_acc: 0.9392\n",
            "Epoch 4/40\n",
            "80651/80651 [==============================] - 64s 788us/step - loss: 0.0616 - acc: 0.9785 - val_loss: 0.0980 - val_acc: 0.9656\n",
            "Epoch 5/40\n",
            "80651/80651 [==============================] - 64s 790us/step - loss: 0.0537 - acc: 0.9818 - val_loss: 0.1062 - val_acc: 0.9634\n",
            "Epoch 6/40\n",
            "80651/80651 [==============================] - 64s 787us/step - loss: 0.0509 - acc: 0.9822 - val_loss: 0.1093 - val_acc: 0.9643\n",
            "Epoch 7/40\n",
            "80651/80651 [==============================] - 64s 789us/step - loss: 0.0484 - acc: 0.9826 - val_loss: 0.1020 - val_acc: 0.9686\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 8/40\n",
            "80651/80651 [==============================] - 63s 785us/step - loss: 0.0289 - acc: 0.9906 - val_loss: 0.0962 - val_acc: 0.9706\n",
            "Epoch 9/40\n",
            "80651/80651 [==============================] - 64s 788us/step - loss: 0.0156 - acc: 0.9962 - val_loss: 0.0980 - val_acc: 0.9708\n",
            "Epoch 10/40\n",
            "80651/80651 [==============================] - 63s 787us/step - loss: 0.0106 - acc: 0.9978 - val_loss: 0.1002 - val_acc: 0.9699\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00010: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njAHg/wNiQ1ES9yZH6QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.997143785300126\n",
            "[[50732  1257]\n",
            " [  847 47978]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/My Drive/Colab Notebooks/data/tokenizer.joblib.gz']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFz63xgMIs1e",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Wvohsm2xt39",
        "colab_type": "code",
        "outputId": "a21badc6-7929-46d3-cfd1-ee7667f6f9bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input\n",
        "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, SpatialDropout1D, Concatenate, Activation\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam, Adamax, Nadam\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "\n",
        "import keras.backend as K\n",
        "if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
        "  from keras.layers import CuDNNLSTM as LSTM\n",
        "  from keras.layers import CuDNNGRU as GRU\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from unicodedata import normalize as normtext\n",
        "from string import punctuation\n",
        "from joblib import dump, load\n",
        "\n",
        "\n",
        "trantab_all = str.maketrans(punctuation, len(punctuation)*\" \")\n",
        "\n",
        "cachedStopWords = list(map(lambda x : normtext('NFKD', x).encode('ASCII', 'ignore').decode('ASCII').lower(),\n",
        "                           stopwords.words(\"portuguese\")))\n",
        "\n",
        "\n",
        "def pp_texto_resultado(text):\n",
        "    if text is not None:\n",
        "        # minusculos e tira acentuacao\n",
        "        text = normtext('NFKD', text).encode('ASCII', 'ignore').decode('ASCII').lower()\n",
        "        text = text.translate(trantab_all) #tira pontuacao\n",
        "        text = re.sub(r'\\d{5,}', '', str(text).strip()) # Retira todo número maior que 5 algarismos\n",
        "        text = remove_stop(text) # retira stopwords\n",
        "    else:\n",
        "        return ''\n",
        "    return text\n",
        "\n",
        "def remove_stop(text):\n",
        "    return ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
        "\n",
        "\n",
        "def preprocess(texts, tokenizer, maxlen=500):\n",
        "  texts = map(pp_texto_resultado, texts)\n",
        "  texts = tokenizer.texts_to_sequences(texts)\n",
        "  return pad_sequences(texts, maxlen=maxlen)\n",
        "\n",
        "\n",
        "def rnn():\n",
        "    embedding_layer = create_embedding_layer()\n",
        "    input_1 = Input(shape=(MAX_SEQUENCE_LENGTH,), name='Texto')\n",
        "    input_2 = Input(shape=(MAX_SEQUENCE_LENGTH,), name='Assunto')\n",
        "    x1 = embedding_layer(input_1)\n",
        "    x2 = embedding_layer(input_2)\n",
        "    x = Concatenate()([x1, x2])\n",
        "    #x = GRU(128, return_sequences=True)(x)\n",
        "    #x = LSTM(128, return_sequences=True)(x)\n",
        "#     x = Dropout(0.2)(x)\n",
        "#     x = Dropout(0.2)(x)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "#     x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    output = Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = Model([input_1, input_2], output)\n",
        "    model.compile(\n",
        "            loss='binary_crossentropy',\n",
        "            optimizer=Nadam(lr=0.01),\n",
        "            metrics=['accuracy']\n",
        "          )\n",
        "    return model\n",
        "  \n",
        "  \n",
        "# some configuration\n",
        "MAX_SEQUENCE_LENGTH = 500\n",
        "VALIDATION_SPLIT = 0.2\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 40\n",
        "\n",
        "datapath = '/content/gdrive/My Drive/Colab Notebooks/data/'\n",
        "\n",
        "\n",
        "embedding_matrix = load(datapath + 'embedding_matrix.joblib.gz')\n",
        "EMBEDDING_DIM = embedding_matrix.shape[1]\n",
        "MAX_VOCAB_SIZE = embedding_matrix.shape[0]\n",
        "print('Found %s by %s word vectors.' % (MAX_VOCAB_SIZE, EMBEDDING_DIM))\n",
        "\n",
        "\n",
        "# prepare text samples and their labels\n",
        "print('Loading in comments...')\n",
        "\n",
        "train = pd.read_excel(datapath + \"train_sentiment.xlsx\", usecols=['texto', 'classify', 'assunto', 'sentimentprob']).dropna()\n",
        "train = train[train[\"classify\"] == 1]\n",
        "texto = train['texto'].apply(pp_texto_resultado)\n",
        "assunto = train['assunto'].apply(pp_texto_resultado)\n",
        "#Loss: binary_crossentropy Activation: sigmoid\n",
        "#targets = train[\"sentimentprob\"].replace(-1, 0).values\n",
        "targets = train[\"sentimentprob\"] > 0.5\n",
        "#Loss: hinge   Activation: tanh\n",
        "#targets = train[\"Sentiment\"].values \n",
        "\n",
        "# convert the sentences (strings) into integers\n",
        "tokenizer = load(datapath + 'tokenizer.joblib.gz')\n",
        "texto_seq = tokenizer.texts_to_sequences(texto)\n",
        "assunto_seq = tokenizer.texts_to_sequences(assunto)\n",
        "\n",
        "\n",
        "# get word -> integer mapping\n",
        "word2idx = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word2idx))\n",
        "\n",
        "\n",
        "# pad sequences so that we get a N x T matrix\n",
        "texto_data = pad_sequences(texto_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "assunto_data = pad_sequences(assunto_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensors:', (texto_data.shape, assunto_data.shape))\n",
        "\n",
        "\n",
        "\n",
        "# load pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = create_embedding_layer()\n",
        "\n",
        "print('Building model...')\n",
        "\n",
        "model = rnn()\n",
        "model.summary()\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=8, verbose=1, restore_best_weights=True)\n",
        "rop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
        "\n",
        "print('Training model...')\n",
        "r = model.fit(\n",
        "  [texto_data, assunto_data],\n",
        "  targets,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  epochs=EPOCHS,\n",
        "  validation_split=VALIDATION_SPLIT,\n",
        "  callbacks=[es, rop]\n",
        ")\n",
        "\n",
        "# plot some data\n",
        "plt.rcParams[\"figure.figsize\"] = (6,4)\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# accuracies\n",
        "plt.plot(r.history['acc'], label='acc')\n",
        "plt.plot(r.history['val_acc'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "p = model.predict([texto_data, assunto_data])\n",
        "print(roc_auc_score(targets, p))\n",
        "pc = np.round(p)\n",
        "print(confusion_matrix(targets, pc))\n",
        "\n",
        "#train['predictions'] = np.where(pc==0, -1, pc) \n",
        "#train['probs'] = p\n",
        "\n",
        "model.save_weights(datapath + 'sentiment_trab_weights.h5')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 11204 by 100 word vectors.\n",
            "Loading in comments...\n",
            "Found 95260 unique tokens.\n",
            "Shape of data tensors: ((48825, 500), (48825, 500))\n",
            "Building model...\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Texto (InputLayer)              (None, 500)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Assunto (InputLayer)            (None, 500)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_11 (Embedding)        (None, 500, 100)     1120400     Texto[0][0]                      \n",
            "                                                                 Assunto[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 500, 200)     0           embedding_11[0][0]               \n",
            "                                                                 embedding_11[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 500, 256)     337920      concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 500, 256)     0           bidirectional_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_5 (GlobalM (None, 256)          0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            257         global_max_pooling1d_5[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 1,458,577\n",
            "Trainable params: 338,177\n",
            "Non-trainable params: 1,120,400\n",
            "__________________________________________________________________________________________________\n",
            "Training model...\n",
            "Train on 39060 samples, validate on 9765 samples\n",
            "Epoch 1/40\n",
            "39060/39060 [==============================] - 34s 868us/step - loss: 0.2430 - acc: 0.8956 - val_loss: 0.1872 - val_acc: 0.9216\n",
            "Epoch 2/40\n",
            "39060/39060 [==============================] - 33s 855us/step - loss: 0.1419 - acc: 0.9440 - val_loss: 0.1585 - val_acc: 0.9357\n",
            "Epoch 3/40\n",
            "39060/39060 [==============================] - 33s 850us/step - loss: 0.1210 - acc: 0.9530 - val_loss: 0.1565 - val_acc: 0.9356\n",
            "Epoch 4/40\n",
            "39060/39060 [==============================] - 33s 852us/step - loss: 0.1074 - acc: 0.9586 - val_loss: 0.1651 - val_acc: 0.9344\n",
            "Epoch 5/40\n",
            "39060/39060 [==============================] - 33s 852us/step - loss: 0.0932 - acc: 0.9637 - val_loss: 0.1908 - val_acc: 0.9231\n",
            "Epoch 6/40\n",
            "39060/39060 [==============================] - 33s 853us/step - loss: 0.1083 - acc: 0.9583 - val_loss: 0.2223 - val_acc: 0.9096\n",
            "Epoch 7/40\n",
            "39060/39060 [==============================] - 33s 849us/step - loss: 0.1148 - acc: 0.9552 - val_loss: 0.2300 - val_acc: 0.9037\n",
            "Epoch 8/40\n",
            "39060/39060 [==============================] - 33s 849us/step - loss: 0.0988 - acc: 0.9615 - val_loss: 0.1984 - val_acc: 0.9231\n",
            "\n",
            "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 9/40\n",
            "39060/39060 [==============================] - 33s 855us/step - loss: 0.0614 - acc: 0.9793 - val_loss: 0.1499 - val_acc: 0.9391\n",
            "Epoch 10/40\n",
            "39060/39060 [==============================] - 33s 851us/step - loss: 0.0443 - acc: 0.9874 - val_loss: 0.1484 - val_acc: 0.9388\n",
            "Epoch 11/40\n",
            "39060/39060 [==============================] - 33s 852us/step - loss: 0.0347 - acc: 0.9917 - val_loss: 0.1488 - val_acc: 0.9398\n",
            "Epoch 12/40\n",
            "39060/39060 [==============================] - 33s 853us/step - loss: 0.0292 - acc: 0.9938 - val_loss: 0.1492 - val_acc: 0.9396\n",
            "Epoch 13/40\n",
            "39060/39060 [==============================] - 33s 847us/step - loss: 0.0245 - acc: 0.9951 - val_loss: 0.1508 - val_acc: 0.9391\n",
            "Epoch 14/40\n",
            "39060/39060 [==============================] - 33s 851us/step - loss: 0.0202 - acc: 0.9961 - val_loss: 0.1516 - val_acc: 0.9410\n",
            "Epoch 15/40\n",
            "39060/39060 [==============================] - 33s 853us/step - loss: 0.0173 - acc: 0.9968 - val_loss: 0.1553 - val_acc: 0.9387\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "Epoch 16/40\n",
            "39060/39060 [==============================] - 33s 852us/step - loss: 0.0142 - acc: 0.9979 - val_loss: 0.1539 - val_acc: 0.9396\n",
            "Epoch 17/40\n",
            "39060/39060 [==============================] - 33s 851us/step - loss: 0.0139 - acc: 0.9979 - val_loss: 0.1542 - val_acc: 0.9398\n",
            "Epoch 18/40\n",
            "39060/39060 [==============================] - 33s 851us/step - loss: 0.0136 - acc: 0.9978 - val_loss: 0.1541 - val_acc: 0.9400\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00018: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUn5Ik79SSvkhTf5KKeWHNPkrpZQf+n8LVy3b2qb4zQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.9975182413187096\n",
            "[[30011   398]\n",
            " [  421 17995]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6ujY1NZI1KQ",
        "colab_type": "text"
      },
      "source": [
        "# Result saving and downloading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMA1UQJ0_tte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "#model.save(datapath + 'model_trabalhista.h5')\n",
        "\n",
        "\n",
        "filenames = ['sentiment_trab_weights.h5', 'classify_trab_weights.h5', 'tokenizer.joblib.gz', 'embedding_matrix.joblib.gz']\n",
        "\n",
        "for name in filenames:\n",
        "  files.download(datapath + name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUdR_7Riz_vT",
        "colab_type": "code",
        "outputId": "8e4879c0-31b2-4656-9293-fa4e8583b249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "datapath + name"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/Colab Notebooks/data/sentiment_trab_weights.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-jnGLlxg9BR",
        "colab_type": "text"
      },
      "source": [
        "# Model Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu1D5V_AClUV",
        "colab_type": "code",
        "outputId": "c5926576-2f23-457c-9f2a-9d2aaef9c0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "from keras.utils import plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the model\n",
        "plot_model(model, to_file='model.png')\n",
        "\n",
        "# Display the image\n",
        "data = plt.imread('model.png')\n",
        "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
        "plt.imshow(data)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAJCCAYAAAA/aldAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGoycTwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQgdu8tlhCHu",
        "colab_type": "code",
        "outputId": "0641a6f6-c0cc-4ed5-83d8-6cb5b269c46d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = preprocess(['improcedeo pleito de aplicacao da multa prevista no artigo 477 da CLT, visto que as verbas rescisorias foram pagas no prazo previsto no mesmo artigo.'],\n",
        "                  tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "ass = preprocess(['multa'], tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "model.predict([text, ass])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5029173]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNXoBxtA6GTm",
        "colab_type": "text"
      },
      "source": [
        "text.shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YHlMtor9M6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def preprocess(texts, tokenizer, maxlen=500):\n",
        "  texts = map(pp_texto_resultado, texts)\n",
        "  texts = tokenizer.texts_to_sequences(texts)\n",
        "  return pad_sequences(texts, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv3NETBD-QZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "planilha = pd.read_excel(datapath + \"decisoes_classificar.xlsx\")\n",
        "texto_data = preprocess(planilha['texto'].values, tokenizer)\n",
        "assunto_data = preprocess(planilha['assunto'].values, tokenizer)\n",
        "\n",
        "\n",
        "p = model.predict([texto_data, assunto_data])\n",
        "pc = np.round(p)\n",
        "planilha['predictions'] = np.where(pc==0, -1, pc) \n",
        "planilha['probs'] = p\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKGog5ifWttb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = model.predict([texto_data, assunto_data])\n",
        "pc = np.round(p)\n",
        "planilha['predictions'] = np.where(pc==0, -1, pc) \n",
        "planilha['probs'] = p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DerfuHHZGICS",
        "colab_type": "code",
        "outputId": "6ffad6de-054c-4215-9539-dcbff4772854",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from joblib import dump\n",
        "dump(embedding_matrix, datapath + 'sent_emb_matrix.joblib.gz', compress='gzip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/My Drive/Colab Notebooks/data/sent_emb_matrix.joblib.gz']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMjET_faEvOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "planilha.to_excel(datapath + 'decisoes_classificadas.xlsx', encoding='utf8', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBCSmw1fGpQT",
        "colab_type": "code",
        "outputId": "d466abc5-1880-4e8b-ff29-035b30926199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 28543
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "\n",
        "def criarRede(optimizer, loss, activation, units, dropout):\n",
        "    # create an LSTM network\n",
        "    input_1 = Input(shape=(MAX_SEQUENCE_LENGTH,), name='Texto')\n",
        "#     input_2 = Input(shape=(MAX_SEQUENCE_LENGTH,), name='Assunto')\n",
        "    x = embedding_layer(input_1)\n",
        "#     x2 = embedding_layer(input_2)\n",
        "#     x = Concatenate()([x1, x2])\n",
        "    x = Bidirectional(LSTM(units=units, return_sequences=True))(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = Bidirectional(LSTM(units=units, return_sequences=True))(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    output = Dense(1, activation=activation)(x)\n",
        "    model = Model(input_1, output)\n",
        "    model.compile(\n",
        "      loss=loss,\n",
        "      optimizer=optimizer(lr=0.01),\n",
        "      metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "classificador = KerasClassifier(build_fn = criarRede)\n",
        "parametros = {'batch_size': [128, 256],\n",
        "              'epochs': [15],\n",
        "              'optimizer': [Adam, Nadam],\n",
        "              'loss': ['binary_crossentropy'],\n",
        "              'activation': ['sigmoid'],\n",
        "              'units': [100, 150],\n",
        "              'dropout': [0.3, 0.5]              \n",
        "             }\n",
        "grid_search = GridSearchCV(estimator = classificador,\n",
        "                           param_grid = parametros,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv=3,\n",
        "                           verbose=2)\n",
        "\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=8, verbose=1, restore_best_weights=True)\n",
        "rop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1)\n",
        "\n",
        "print('Training model...')\n",
        "grid_search = grid_search.fit(\n",
        "  texto_data,\n",
        "  targets,\n",
        "  validation_split=VALIDATION_SPLIT,\n",
        "  callbacks=[es, rop]\n",
        ")\n",
        "\n",
        "melhores_parametros = grid_search.best_params_\n",
        "melhor_precisao = grid_search.best_score_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.6895 - acc: 0.6430 - val_loss: 0.4930 - val_acc: 0.7720\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.3637 - acc: 0.8411 - val_loss: 0.3099 - val_acc: 0.8718\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1986 - acc: 0.9291 - val_loss: 0.2164 - val_acc: 0.9255\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1265 - acc: 0.9556 - val_loss: 0.2121 - val_acc: 0.9270\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1041 - acc: 0.9634 - val_loss: 0.1855 - val_acc: 0.9344\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0557 - acc: 0.9806 - val_loss: 0.1799 - val_acc: 0.9419\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0293 - acc: 0.9918 - val_loss: 0.1925 - val_acc: 0.9374\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0222 - acc: 0.9948 - val_loss: 0.1779 - val_acc: 0.9404\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1754 - acc: 0.9448 - val_loss: 0.1785 - val_acc: 0.9389\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0626 - acc: 0.9776 - val_loss: 0.1790 - val_acc: 0.9449\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0238 - acc: 0.9929 - val_loss: 0.2402 - val_acc: 0.9255\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0188 - acc: 0.9948 - val_loss: 0.2255 - val_acc: 0.9329\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0163 - acc: 0.9952 - val_loss: 0.2698 - val_acc: 0.9106\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0097 - acc: 0.9974 - val_loss: 0.2188 - val_acc: 0.9404\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 0.2140 - val_acc: 0.9449\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100, total= 1.1min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 7s 2ms/step - loss: 0.6401 - acc: 0.6662 - val_loss: 0.4605 - val_acc: 0.7630\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.3505 - acc: 0.8497 - val_loss: 0.2887 - val_acc: 0.8808\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1955 - acc: 0.9179 - val_loss: 0.2624 - val_acc: 0.8972\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1205 - acc: 0.9556 - val_loss: 0.2412 - val_acc: 0.9136\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0787 - acc: 0.9724 - val_loss: 0.2265 - val_acc: 0.9285\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0586 - acc: 0.9787 - val_loss: 0.2053 - val_acc: 0.9285\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0360 - acc: 0.9892 - val_loss: 0.2193 - val_acc: 0.9314\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0266 - acc: 0.9910 - val_loss: 0.2079 - val_acc: 0.9300\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0217 - acc: 0.9929 - val_loss: 0.2107 - val_acc: 0.9389\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0158 - acc: 0.9944 - val_loss: 0.2452 - val_acc: 0.9225\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0891 - acc: 0.9739 - val_loss: 0.2247 - val_acc: 0.9180\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0566 - acc: 0.9817 - val_loss: 0.2091 - val_acc: 0.9300\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0234 - acc: 0.9948 - val_loss: 0.2046 - val_acc: 0.9344\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0157 - acc: 0.9981 - val_loss: 0.2058 - val_acc: 0.9374\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0111 - acc: 0.9985 - val_loss: 0.2024 - val_acc: 0.9329\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100, total= 1.1min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 7s 3ms/step - loss: 0.6826 - acc: 0.6412 - val_loss: 0.5048 - val_acc: 0.7720\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.4136 - acc: 0.8139 - val_loss: 0.3758 - val_acc: 0.8316\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.2194 - acc: 0.9138 - val_loss: 0.2101 - val_acc: 0.9285\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1210 - acc: 0.9571 - val_loss: 0.1769 - val_acc: 0.9374\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0648 - acc: 0.9780 - val_loss: 0.1826 - val_acc: 0.9255\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0509 - acc: 0.9866 - val_loss: 0.1867 - val_acc: 0.9300\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0389 - acc: 0.9884 - val_loss: 0.1262 - val_acc: 0.9493\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0304 - acc: 0.9925 - val_loss: 0.1422 - val_acc: 0.9478\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0200 - acc: 0.9948 - val_loss: 0.1302 - val_acc: 0.9478\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0167 - acc: 0.9970 - val_loss: 0.1490 - val_acc: 0.9404\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0091 - acc: 0.9981 - val_loss: 0.1839 - val_acc: 0.9359\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0115 - acc: 0.9978 - val_loss: 0.1693 - val_acc: 0.9463\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0101 - acc: 0.9981 - val_loss: 0.1811 - val_acc: 0.9434\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0039 - acc: 0.9996 - val_loss: 0.1705 - val_acc: 0.9434\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0033 - acc: 0.9996 - val_loss: 0.1698 - val_acc: 0.9449\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00015: early stopping\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100, total= 1.1min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 9s 3ms/step - loss: 0.7937 - acc: 0.6184 - val_loss: 0.5281 - val_acc: 0.7422\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.4192 - acc: 0.7993 - val_loss: 0.4451 - val_acc: 0.7720\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.2707 - acc: 0.8900 - val_loss: 0.3103 - val_acc: 0.8867\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1466 - acc: 0.9459 - val_loss: 0.2128 - val_acc: 0.9151\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0755 - acc: 0.9758 - val_loss: 0.2001 - val_acc: 0.9300\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0450 - acc: 0.9840 - val_loss: 0.1972 - val_acc: 0.9344\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0243 - acc: 0.9929 - val_loss: 0.2217 - val_acc: 0.9255\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0208 - acc: 0.9955 - val_loss: 0.2067 - val_acc: 0.9344\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0556 - acc: 0.9780 - val_loss: 0.2729 - val_acc: 0.9016\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0586 - acc: 0.9814 - val_loss: 0.1858 - val_acc: 0.9404\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0423 - acc: 0.9843 - val_loss: 0.2587 - val_acc: 0.9270\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0357 - acc: 0.9877 - val_loss: 0.2109 - val_acc: 0.9285\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0162 - acc: 0.9952 - val_loss: 0.2111 - val_acc: 0.9314\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0056 - acc: 0.9993 - val_loss: 0.2329 - val_acc: 0.9195\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.2232 - val_acc: 0.9329\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150, total= 1.6min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 9s 3ms/step - loss: 0.7630 - acc: 0.6225 - val_loss: 0.5714 - val_acc: 0.6960\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.4482 - acc: 0.7889 - val_loss: 0.4608 - val_acc: 0.7943\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.2934 - acc: 0.8810 - val_loss: 0.3006 - val_acc: 0.8793\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1669 - acc: 0.9377 - val_loss: 0.2316 - val_acc: 0.9121\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0788 - acc: 0.9743 - val_loss: 0.2118 - val_acc: 0.9195\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0425 - acc: 0.9866 - val_loss: 0.2511 - val_acc: 0.9061\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0327 - acc: 0.9910 - val_loss: 0.2223 - val_acc: 0.9180\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0187 - acc: 0.9963 - val_loss: 0.2662 - val_acc: 0.9106\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0112 - acc: 0.9978 - val_loss: 0.2528 - val_acc: 0.9046\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0089 - acc: 0.9989 - val_loss: 0.3887 - val_acc: 0.8852\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0078 - acc: 0.9981 - val_loss: 0.2658 - val_acc: 0.9136\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.2655 - val_acc: 0.9106\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.2670 - val_acc: 0.9106\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00013: early stopping\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150, total= 1.4min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 10s 4ms/step - loss: 0.8079 - acc: 0.6289 - val_loss: 0.5259 - val_acc: 0.7630\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.4754 - acc: 0.7743 - val_loss: 0.4198 - val_acc: 0.8182\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.2942 - acc: 0.8866 - val_loss: 0.2721 - val_acc: 0.9016\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1679 - acc: 0.9351 - val_loss: 0.2097 - val_acc: 0.9195\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0973 - acc: 0.9675 - val_loss: 0.1900 - val_acc: 0.9285\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0692 - acc: 0.9717 - val_loss: 0.1900 - val_acc: 0.9240\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0454 - acc: 0.9840 - val_loss: 0.2001 - val_acc: 0.9225\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0248 - acc: 0.9929 - val_loss: 0.1874 - val_acc: 0.9240\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0139 - acc: 0.9966 - val_loss: 0.1851 - val_acc: 0.9329\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0126 - acc: 0.9981 - val_loss: 0.1911 - val_acc: 0.9404\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0094 - acc: 0.9966 - val_loss: 0.1753 - val_acc: 0.9449\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0061 - acc: 0.9993 - val_loss: 0.2333 - val_acc: 0.9285\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0103 - acc: 0.9985 - val_loss: 0.1828 - val_acc: 0.9285\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0209 - acc: 0.9937 - val_loss: 0.1853 - val_acc: 0.9419\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0481 - acc: 0.9802 - val_loss: 0.1879 - val_acc: 0.9314\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150, total= 1.6min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 8s 3ms/step - loss: 0.7584 - acc: 0.6057 - val_loss: 0.5295 - val_acc: 0.7139\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.5165 - acc: 0.7482 - val_loss: 0.3957 - val_acc: 0.8390\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.3118 - acc: 0.8732 - val_loss: 0.3127 - val_acc: 0.8748\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1619 - acc: 0.9437 - val_loss: 0.2231 - val_acc: 0.9180\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1260 - acc: 0.9567 - val_loss: 0.3255 - val_acc: 0.8778\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0637 - acc: 0.9814 - val_loss: 0.2219 - val_acc: 0.9151\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0241 - acc: 0.9955 - val_loss: 0.2200 - val_acc: 0.9195\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0125 - acc: 0.9970 - val_loss: 0.2214 - val_acc: 0.9195\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.2351 - val_acc: 0.9180\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.2465 - val_acc: 0.9165\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.2577 - val_acc: 0.9136\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2675 - val_acc: 0.9121\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 9.4559e-04 - acc: 1.0000 - val_loss: 0.2686 - val_acc: 0.9121\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 9.2249e-04 - acc: 1.0000 - val_loss: 0.2693 - val_acc: 0.9121\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 9.2053e-04 - acc: 1.0000 - val_loss: 0.2698 - val_acc: 0.9121\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00015: early stopping\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 8s 3ms/step - loss: 0.7959 - acc: 0.5938 - val_loss: 0.5408 - val_acc: 0.7288\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.5355 - acc: 0.7385 - val_loss: 0.4514 - val_acc: 0.8182\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.3010 - acc: 0.8810 - val_loss: 0.3540 - val_acc: 0.8495\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.2466 - acc: 0.9034 - val_loss: 0.2796 - val_acc: 0.8927\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1216 - acc: 0.9575 - val_loss: 0.2017 - val_acc: 0.9225\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0723 - acc: 0.9776 - val_loss: 0.2320 - val_acc: 0.9180\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0328 - acc: 0.9933 - val_loss: 0.2064 - val_acc: 0.9285\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0141 - acc: 0.9985 - val_loss: 0.2174 - val_acc: 0.9225\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0071 - acc: 0.9993 - val_loss: 0.2273 - val_acc: 0.9255\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0040 - acc: 0.9993 - val_loss: 0.2470 - val_acc: 0.9210\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.2485 - val_acc: 0.9210\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.2492 - val_acc: 0.9210\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.2511 - val_acc: 0.9225\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00013: early stopping\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100, total= 1.1min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 9s 3ms/step - loss: 0.7304 - acc: 0.6345 - val_loss: 0.4903 - val_acc: 0.7824\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.4714 - acc: 0.7848 - val_loss: 0.4302 - val_acc: 0.7988\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.2518 - acc: 0.8982 - val_loss: 0.2563 - val_acc: 0.9046\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1452 - acc: 0.9489 - val_loss: 0.1624 - val_acc: 0.9463\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0940 - acc: 0.9661 - val_loss: 0.1573 - val_acc: 0.9419\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0516 - acc: 0.9836 - val_loss: 0.1838 - val_acc: 0.9359\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0239 - acc: 0.9948 - val_loss: 0.1357 - val_acc: 0.9434\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0141 - acc: 0.9966 - val_loss: 0.1533 - val_acc: 0.9434\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.9196 - acc: 0.7676 - val_loss: 0.6926 - val_acc: 0.5365\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.7034 - acc: 0.5632 - val_loss: 0.7239 - val_acc: 0.5410\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.6178 - acc: 0.6397 - val_loss: 0.5714 - val_acc: 0.6110\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.4236 - acc: 0.8142 - val_loss: 0.3123 - val_acc: 0.9001\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.2472 - acc: 0.9179 - val_loss: 0.2800 - val_acc: 0.9255\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.2172 - acc: 0.9332 - val_loss: 0.2545 - val_acc: 0.9285\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1919 - acc: 0.9332 - val_loss: 0.2359 - val_acc: 0.9359\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00015: early stopping\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 11s 4ms/step - loss: 0.9006 - acc: 0.5990 - val_loss: 0.5723 - val_acc: 0.7511\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.5174 - acc: 0.7523 - val_loss: 0.4718 - val_acc: 0.7988\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.3241 - acc: 0.8650 - val_loss: 0.2538 - val_acc: 0.9180\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1427 - acc: 0.9478 - val_loss: 0.2205 - val_acc: 0.9225\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.6620 - acc: 0.8087 - val_loss: 0.3510 - val_acc: 0.8614\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1810 - acc: 0.9347 - val_loss: 0.2005 - val_acc: 0.9359\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0883 - acc: 0.9735 - val_loss: 0.1770 - val_acc: 0.9389\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0455 - acc: 0.9869 - val_loss: 0.1768 - val_acc: 0.9404\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0327 - acc: 0.9903 - val_loss: 0.1942 - val_acc: 0.9344\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0159 - acc: 0.9955 - val_loss: 0.1853 - val_acc: 0.9449\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0074 - acc: 0.9985 - val_loss: 0.1967 - val_acc: 0.9329\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0040 - acc: 0.9996 - val_loss: 0.1988 - val_acc: 0.9374\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.2093 - val_acc: 0.9344\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.2111 - val_acc: 0.9344\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.2126 - val_acc: 0.9344\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150, total= 1.6min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 11s 4ms/step - loss: 0.8880 - acc: 0.6050 - val_loss: 0.7176 - val_acc: 0.5678\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.5442 - acc: 0.7262 - val_loss: 0.4399 - val_acc: 0.7824\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.4188 - acc: 0.8064 - val_loss: 0.3645 - val_acc: 0.8599\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.2254 - acc: 0.9131 - val_loss: 0.2677 - val_acc: 0.8867\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1494 - acc: 0.9426 - val_loss: 0.2417 - val_acc: 0.8897\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0843 - acc: 0.9698 - val_loss: 0.2544 - val_acc: 0.8957\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0359 - acc: 0.9910 - val_loss: 0.2428 - val_acc: 0.9046\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0135 - acc: 0.9989 - val_loss: 0.2442 - val_acc: 0.9076\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 1.4056 - acc: 0.6255 - val_loss: 0.7079 - val_acc: 0.4262\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.6335 - acc: 0.6084 - val_loss: 0.5337 - val_acc: 0.8092\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.4584 - acc: 0.7993 - val_loss: 0.4595 - val_acc: 0.8033\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.4119 - acc: 0.8213 - val_loss: 0.4346 - val_acc: 0.8092\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.3714 - acc: 0.8515 - val_loss: 0.3944 - val_acc: 0.8480\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00013: early stopping\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150, total= 1.5min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 11s 4ms/step - loss: 0.8961 - acc: 0.6173 - val_loss: 0.6877 - val_acc: 0.5484\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.5401 - acc: 0.7430 - val_loss: 0.5020 - val_acc: 0.7273\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.3738 - acc: 0.8325 - val_loss: 0.2706 - val_acc: 0.9136\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1837 - acc: 0.9306 - val_loss: 0.2119 - val_acc: 0.9285\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1102 - acc: 0.9601 - val_loss: 0.2319 - val_acc: 0.9016\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0605 - acc: 0.9772 - val_loss: 0.1643 - val_acc: 0.9359\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0426 - acc: 0.9858 - val_loss: 0.9299 - val_acc: 0.7466\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 1.4369 - acc: 0.4961 - val_loss: 0.9066 - val_acc: 0.4605\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.7314 - acc: 0.5121 - val_loss: 1.1449 - val_acc: 0.4605\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.7397 - acc: 0.5155 - val_loss: 1.3168 - val_acc: 0.4605\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.7313 - acc: 0.5192 - val_loss: 0.9897 - val_acc: 0.4605\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.6881 - acc: 0.5636 - val_loss: 1.1234 - val_acc: 0.4605\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.6846 - acc: 0.5636 - val_loss: 1.1284 - val_acc: 0.4605\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.6827 - acc: 0.5651 - val_loss: 1.1396 - val_acc: 0.4605\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00014: early stopping\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150, total= 1.6min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 10s 4ms/step - loss: 0.7733 - acc: 0.6255 - val_loss: 0.5604 - val_acc: 0.7452\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.4306 - acc: 0.8031 - val_loss: 0.4101 - val_acc: 0.8376\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.2461 - acc: 0.9008 - val_loss: 0.3072 - val_acc: 0.8778\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1617 - acc: 0.9351 - val_loss: 0.2817 - val_acc: 0.9016\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1159 - acc: 0.9575 - val_loss: 0.2365 - val_acc: 0.9106\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0733 - acc: 0.9724 - val_loss: 0.2440 - val_acc: 0.9091\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0429 - acc: 0.9851 - val_loss: 0.2522 - val_acc: 0.9195\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0401 - acc: 0.9888 - val_loss: 0.2345 - val_acc: 0.9180\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0307 - acc: 0.9918 - val_loss: 0.2363 - val_acc: 0.9061\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0227 - acc: 0.9933 - val_loss: 0.2291 - val_acc: 0.9225\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0616 - acc: 0.9772 - val_loss: 0.2721 - val_acc: 0.9061\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0436 - acc: 0.9855 - val_loss: 0.2605 - val_acc: 0.9195\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0179 - acc: 0.9970 - val_loss: 0.2486 - val_acc: 0.9240\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0076 - acc: 0.9985 - val_loss: 0.2498 - val_acc: 0.9255\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0081 - acc: 0.9981 - val_loss: 0.2236 - val_acc: 0.9314\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 10s 4ms/step - loss: 0.7945 - acc: 0.6158 - val_loss: 0.5706 - val_acc: 0.7049\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.4755 - acc: 0.7710 - val_loss: 0.4904 - val_acc: 0.7958\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.2925 - acc: 0.8840 - val_loss: 0.3519 - val_acc: 0.8927\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1718 - acc: 0.9370 - val_loss: 0.2692 - val_acc: 0.8972\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1290 - acc: 0.9470 - val_loss: 0.2571 - val_acc: 0.9225\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0828 - acc: 0.9679 - val_loss: 0.2171 - val_acc: 0.9255\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0426 - acc: 0.9858 - val_loss: 0.2138 - val_acc: 0.9300\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0195 - acc: 0.9948 - val_loss: 0.2231 - val_acc: 0.9255\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0141 - acc: 0.9970 - val_loss: 0.2361 - val_acc: 0.9210\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0114 - acc: 0.9981 - val_loss: 0.2177 - val_acc: 0.9106\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0263 - acc: 0.9910 - val_loss: 0.2290 - val_acc: 0.9195\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0285 - acc: 0.9907 - val_loss: 0.2252 - val_acc: 0.9240\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0109 - acc: 0.9981 - val_loss: 0.2140 - val_acc: 0.9300\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0073 - acc: 0.9989 - val_loss: 0.2072 - val_acc: 0.9329\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0050 - acc: 0.9996 - val_loss: 0.2058 - val_acc: 0.9314\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 10s 4ms/step - loss: 0.7493 - acc: 0.6281 - val_loss: 0.5257 - val_acc: 0.7630\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.4035 - acc: 0.8310 - val_loss: 0.4178 - val_acc: 0.8435\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.2545 - acc: 0.9004 - val_loss: 0.2416 - val_acc: 0.9270\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1648 - acc: 0.9344 - val_loss: 0.1973 - val_acc: 0.9478\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0996 - acc: 0.9620 - val_loss: 0.1757 - val_acc: 0.9478\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0616 - acc: 0.9791 - val_loss: 0.1868 - val_acc: 0.9374\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0723 - acc: 0.9724 - val_loss: 0.1874 - val_acc: 0.9270\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0456 - acc: 0.9836 - val_loss: 0.1775 - val_acc: 0.9314\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1256 - acc: 0.9549 - val_loss: 0.2224 - val_acc: 0.9106\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0715 - acc: 0.9746 - val_loss: 0.1609 - val_acc: 0.9389\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0328 - acc: 0.9918 - val_loss: 0.1606 - val_acc: 0.9404\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0234 - acc: 0.9899 - val_loss: 0.1543 - val_acc: 0.9419\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0181 - acc: 0.9940 - val_loss: 0.1518 - val_acc: 0.9478\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0141 - acc: 0.9959 - val_loss: 0.1495 - val_acc: 0.9508\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0059 - acc: 0.9989 - val_loss: 0.1632 - val_acc: 0.9493\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 13s 5ms/step - loss: 0.8865 - acc: 0.6125 - val_loss: 0.5528 - val_acc: 0.7571\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.4420 - acc: 0.7978 - val_loss: 0.4695 - val_acc: 0.8256\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.2522 - acc: 0.9004 - val_loss: 0.2956 - val_acc: 0.8823\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1372 - acc: 0.9474 - val_loss: 0.2599 - val_acc: 0.9046\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1445 - acc: 0.9463 - val_loss: 0.2748 - val_acc: 0.9016\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0793 - acc: 0.9672 - val_loss: 0.2155 - val_acc: 0.9300\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0463 - acc: 0.9825 - val_loss: 0.2235 - val_acc: 0.9151\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0334 - acc: 0.9907 - val_loss: 0.2287 - val_acc: 0.9195\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0149 - acc: 0.9959 - val_loss: 0.2077 - val_acc: 0.9210\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0106 - acc: 0.9981 - val_loss: 0.2482 - val_acc: 0.8987\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0109 - acc: 0.9966 - val_loss: 0.2104 - val_acc: 0.9255\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0146 - acc: 0.9948 - val_loss: 0.2075 - val_acc: 0.9285\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0182 - acc: 0.9959 - val_loss: 0.2001 - val_acc: 0.9240\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0146 - acc: 0.9952 - val_loss: 0.2184 - val_acc: 0.9180\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0063 - acc: 0.9985 - val_loss: 0.2276 - val_acc: 0.9255\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150, total= 1.6min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 13s 5ms/step - loss: 0.8947 - acc: 0.5830 - val_loss: 0.6095 - val_acc: 0.6855\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.4753 - acc: 0.7699 - val_loss: 0.4767 - val_acc: 0.8122\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.2986 - acc: 0.8784 - val_loss: 0.3688 - val_acc: 0.8823\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1896 - acc: 0.9276 - val_loss: 0.2746 - val_acc: 0.9106\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1462 - acc: 0.9433 - val_loss: 0.2631 - val_acc: 0.8927\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0894 - acc: 0.9627 - val_loss: 0.2184 - val_acc: 0.9210\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0423 - acc: 0.9866 - val_loss: 0.2084 - val_acc: 0.9329\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0230 - acc: 0.9940 - val_loss: 0.2069 - val_acc: 0.9300\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0139 - acc: 0.9966 - val_loss: 0.2163 - val_acc: 0.9121\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0074 - acc: 0.9981 - val_loss: 0.2524 - val_acc: 0.9180\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0078 - acc: 0.9978 - val_loss: 0.2260 - val_acc: 0.9225\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0128 - acc: 0.9970 - val_loss: 0.2362 - val_acc: 0.9270\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0100 - acc: 0.9978 - val_loss: 0.2299 - val_acc: 0.9270\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0105 - acc: 0.9970 - val_loss: 0.2195 - val_acc: 0.9344\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.2197 - val_acc: 0.9329\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150, total= 1.7min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 13s 5ms/step - loss: 1.0597 - acc: 0.5882 - val_loss: 0.5568 - val_acc: 0.7630\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.4604 - acc: 0.7934 - val_loss: 0.4458 - val_acc: 0.8480\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.3148 - acc: 0.8717 - val_loss: 0.3343 - val_acc: 0.8733\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1913 - acc: 0.9258 - val_loss: 0.2732 - val_acc: 0.9180\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1252 - acc: 0.9519 - val_loss: 0.2291 - val_acc: 0.9240\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0777 - acc: 0.9687 - val_loss: 0.2034 - val_acc: 0.9300\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0396 - acc: 0.9896 - val_loss: 0.2041 - val_acc: 0.9180\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0355 - acc: 0.9892 - val_loss: 0.2683 - val_acc: 0.9046\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0413 - acc: 0.9858 - val_loss: 0.1853 - val_acc: 0.9240\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0297 - acc: 0.9892 - val_loss: 0.1901 - val_acc: 0.9240\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0214 - acc: 0.9929 - val_loss: 0.1563 - val_acc: 0.9449\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0147 - acc: 0.9952 - val_loss: 0.2082 - val_acc: 0.9225\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0326 - acc: 0.9899 - val_loss: 0.1762 - val_acc: 0.9344\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0424 - acc: 0.9862 - val_loss: 0.1780 - val_acc: 0.9344\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0307 - acc: 0.9877 - val_loss: 0.1901 - val_acc: 0.9314\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150, total= 1.7min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 12s 5ms/step - loss: 0.8341 - acc: 0.6106 - val_loss: 0.5641 - val_acc: 0.7213\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.5230 - acc: 0.7452 - val_loss: 0.7739 - val_acc: 0.4903\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.3908 - acc: 0.8314 - val_loss: 0.3457 - val_acc: 0.8838\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1846 - acc: 0.9310 - val_loss: 0.3883 - val_acc: 0.8271\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1523 - acc: 0.9426 - val_loss: 0.2450 - val_acc: 0.9121\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0827 - acc: 0.9698 - val_loss: 0.2542 - val_acc: 0.9001\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0449 - acc: 0.9862 - val_loss: 0.2095 - val_acc: 0.9106\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0177 - acc: 0.9970 - val_loss: 0.2215 - val_acc: 0.9225\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0127 - acc: 0.9966 - val_loss: 0.2073 - val_acc: 0.9300\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0084 - acc: 0.9993 - val_loss: 0.2057 - val_acc: 0.9255\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0103 - acc: 0.9974 - val_loss: 0.2064 - val_acc: 0.9270\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0085 - acc: 0.9989 - val_loss: 0.1981 - val_acc: 0.9270\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0076 - acc: 0.9981 - val_loss: 0.2519 - val_acc: 0.9180\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.5492 - acc: 0.9239 - val_loss: 0.9101 - val_acc: 0.3785\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.7376 - acc: 0.5375 - val_loss: 0.6608 - val_acc: 0.6259\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 12s 4ms/step - loss: 0.8225 - acc: 0.6046 - val_loss: 0.5673 - val_acc: 0.6617\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.5547 - acc: 0.7113 - val_loss: 0.4580 - val_acc: 0.7779\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.4143 - acc: 0.8161 - val_loss: 0.4083 - val_acc: 0.8644\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.2391 - acc: 0.9135 - val_loss: 0.2967 - val_acc: 0.9121\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1274 - acc: 0.9530 - val_loss: 0.2688 - val_acc: 0.9151\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1068 - acc: 0.9612 - val_loss: 0.2392 - val_acc: 0.9091\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0534 - acc: 0.9847 - val_loss: 0.2188 - val_acc: 0.9270\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0303 - acc: 0.9929 - val_loss: 0.2330 - val_acc: 0.9240\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0184 - acc: 0.9966 - val_loss: 0.2284 - val_acc: 0.9046\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0067 - acc: 0.9996 - val_loss: 0.2261 - val_acc: 0.9195\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0077 - acc: 0.9981 - val_loss: 0.2279 - val_acc: 0.9165\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.4008 - acc: 0.9079 - val_loss: 0.6344 - val_acc: 0.6230\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.6117 - acc: 0.6195 - val_loss: 0.5772 - val_acc: 0.6200\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.5046 - acc: 0.7669 - val_loss: 0.4902 - val_acc: 0.7928\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.4294 - acc: 0.8087 - val_loss: 0.4547 - val_acc: 0.8376\n",
            "Restoring model weights from the end of the best epoch\n",
            "Epoch 00015: early stopping\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100, total= 1.3min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 12s 5ms/step - loss: 0.8253 - acc: 0.5946 - val_loss: 0.5619 - val_acc: 0.7809\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.5606 - acc: 0.7393 - val_loss: 0.5254 - val_acc: 0.7705\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.3708 - acc: 0.8422 - val_loss: 0.3392 - val_acc: 0.9031\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.3591 - acc: 0.8568 - val_loss: 0.2948 - val_acc: 0.9165\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1569 - acc: 0.9433 - val_loss: 0.2070 - val_acc: 0.9478\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.1035 - acc: 0.9642 - val_loss: 0.1795 - val_acc: 0.9449\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0608 - acc: 0.9765 - val_loss: 0.1701 - val_acc: 0.9419\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0297 - acc: 0.9944 - val_loss: 0.1642 - val_acc: 0.9463\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0200 - acc: 0.9940 - val_loss: 0.1788 - val_acc: 0.9419\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0135 - acc: 0.9974 - val_loss: 0.1791 - val_acc: 0.9404\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0071 - acc: 0.9993 - val_loss: 0.1694 - val_acc: 0.9389\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.0055 - acc: 0.9996 - val_loss: 0.1697 - val_acc: 0.9493\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.9955 - acc: 0.8176 - val_loss: 0.3618 - val_acc: 0.9031\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.2773 - acc: 0.9041 - val_loss: 0.3297 - val_acc: 0.9091\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 4s 1ms/step - loss: 0.2411 - acc: 0.9112 - val_loss: 0.3048 - val_acc: 0.9121\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100, total= 1.3min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 14s 5ms/step - loss: 1.0959 - acc: 0.5662 - val_loss: 0.5903 - val_acc: 0.7049\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.5548 - acc: 0.7244 - val_loss: 0.5007 - val_acc: 0.7735\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.3911 - acc: 0.8381 - val_loss: 0.3872 - val_acc: 0.8554\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.3087 - acc: 0.8717 - val_loss: 0.2957 - val_acc: 0.8912\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.2029 - acc: 0.9235 - val_loss: 0.2733 - val_acc: 0.8912\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0975 - acc: 0.9687 - val_loss: 0.2458 - val_acc: 0.9121\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0375 - acc: 0.9910 - val_loss: 0.2122 - val_acc: 0.9270\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0161 - acc: 0.9978 - val_loss: 0.2002 - val_acc: 0.9225\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0353 - acc: 0.9910 - val_loss: 2.5320 - val_acc: 0.6215\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 1.4134 - acc: 0.5699 - val_loss: 0.5603 - val_acc: 0.7407\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.4481 - acc: 0.7896 - val_loss: 0.3673 - val_acc: 0.9151\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.2626 - acc: 0.9068 - val_loss: 0.3307 - val_acc: 0.9091\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.2013 - acc: 0.9291 - val_loss: 0.2765 - val_acc: 0.9225\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1342 - acc: 0.9575 - val_loss: 0.2735 - val_acc: 0.9180\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1172 - acc: 0.9649 - val_loss: 0.2681 - val_acc: 0.9151\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150, total= 1.7min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 15s 5ms/step - loss: 1.1985 - acc: 0.5502 - val_loss: 0.5702 - val_acc: 0.7228\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.5846 - acc: 0.7117 - val_loss: 0.5041 - val_acc: 0.7779\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.4524 - acc: 0.7769 - val_loss: 0.4008 - val_acc: 0.8435\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.2485 - acc: 0.8952 - val_loss: 0.3169 - val_acc: 0.8703\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.5523 - acc: 0.7859 - val_loss: 0.3287 - val_acc: 0.8927\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1707 - acc: 0.9284 - val_loss: 0.2287 - val_acc: 0.9493\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0956 - acc: 0.9690 - val_loss: 0.2245 - val_acc: 0.9210\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0529 - acc: 0.9896 - val_loss: 0.2295 - val_acc: 0.9300\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0289 - acc: 0.9929 - val_loss: 0.1964 - val_acc: 0.9374\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0183 - acc: 0.9970 - val_loss: 0.1935 - val_acc: 0.9344\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0121 - acc: 0.9978 - val_loss: 0.2312 - val_acc: 0.9210\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0078 - acc: 0.9985 - val_loss: 0.2000 - val_acc: 0.9404\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0037 - acc: 0.9996 - val_loss: 0.2030 - val_acc: 0.9389\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0021 - acc: 0.9996 - val_loss: 0.2067 - val_acc: 0.9344\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.2114 - val_acc: 0.9329\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150, total= 1.8min\n",
            "[CV] activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 16s 6ms/step - loss: 1.1099 - acc: 0.5882 - val_loss: 0.6232 - val_acc: 0.5797\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.5582 - acc: 0.7188 - val_loss: 0.5370 - val_acc: 0.6602\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.4081 - acc: 0.8128 - val_loss: 0.3621 - val_acc: 0.8718\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.2267 - acc: 0.9146 - val_loss: 0.2491 - val_acc: 0.9240\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.9133 - acc: 0.7452 - val_loss: 0.5568 - val_acc: 0.8942\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.3475 - acc: 0.8915 - val_loss: 0.3718 - val_acc: 0.9106\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.2168 - acc: 0.9269 - val_loss: 0.3099 - val_acc: 0.9300\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1629 - acc: 0.9452 - val_loss: 0.2405 - val_acc: 0.9344\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1813 - acc: 0.9373 - val_loss: 0.2743 - val_acc: 0.9165\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1154 - acc: 0.9620 - val_loss: 0.2281 - val_acc: 0.9374\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0912 - acc: 0.9702 - val_loss: 0.2294 - val_acc: 0.9255\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0827 - acc: 0.9731 - val_loss: 0.2003 - val_acc: 0.9359\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.1500 - acc: 0.9459 - val_loss: 0.2105 - val_acc: 0.9329\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0723 - acc: 0.9758 - val_loss: 0.2037 - val_acc: 0.9419\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 6s 2ms/step - loss: 0.0655 - acc: 0.9795 - val_loss: 0.1926 - val_acc: 0.9374\n",
            "[CV]  activation=sigmoid, batch_size=128, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150, total= 1.7min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 13s 5ms/step - loss: 0.8159 - acc: 0.5748 - val_loss: 0.6067 - val_acc: 0.6975\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5177 - acc: 0.7497 - val_loss: 0.5088 - val_acc: 0.7392\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.4055 - acc: 0.8191 - val_loss: 0.4168 - val_acc: 0.8122\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2786 - acc: 0.8840 - val_loss: 0.3267 - val_acc: 0.8689\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1613 - acc: 0.9399 - val_loss: 0.2776 - val_acc: 0.8867\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1105 - acc: 0.9631 - val_loss: 0.2575 - val_acc: 0.8972\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0632 - acc: 0.9795 - val_loss: 0.2588 - val_acc: 0.8972\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0382 - acc: 0.9888 - val_loss: 0.3267 - val_acc: 0.8703\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2317 - acc: 0.8978 - val_loss: 0.3626 - val_acc: 0.8793\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2122 - acc: 0.9206 - val_loss: 0.2452 - val_acc: 0.9031\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1369 - acc: 0.9504 - val_loss: 0.2223 - val_acc: 0.9225\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0910 - acc: 0.9657 - val_loss: 0.2229 - val_acc: 0.9151\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0597 - acc: 0.9787 - val_loss: 0.2051 - val_acc: 0.9151\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0369 - acc: 0.9881 - val_loss: 0.3086 - val_acc: 0.8942\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0248 - acc: 0.9918 - val_loss: 0.3001 - val_acc: 0.9046\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100, total= 1.1min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 14s 5ms/step - loss: 0.7815 - acc: 0.5647 - val_loss: 0.6196 - val_acc: 0.6692\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.4940 - acc: 0.7646 - val_loss: 0.4947 - val_acc: 0.7615\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.3524 - acc: 0.8478 - val_loss: 0.4084 - val_acc: 0.8137\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2241 - acc: 0.9142 - val_loss: 0.2738 - val_acc: 0.8808\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1422 - acc: 0.9448 - val_loss: 0.2941 - val_acc: 0.8778\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1175 - acc: 0.9564 - val_loss: 0.2470 - val_acc: 0.9061\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0739 - acc: 0.9746 - val_loss: 0.2179 - val_acc: 0.9285\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0418 - acc: 0.9910 - val_loss: 0.2233 - val_acc: 0.9136\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0239 - acc: 0.9918 - val_loss: 0.2344 - val_acc: 0.9151\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0163 - acc: 0.9974 - val_loss: 0.2762 - val_acc: 0.8957\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.2771 - val_acc: 0.9031\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0147 - acc: 0.9978 - val_loss: 0.1997 - val_acc: 0.9434\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0089 - acc: 0.9985 - val_loss: 0.2715 - val_acc: 0.9136\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0108 - acc: 0.9966 - val_loss: 0.2301 - val_acc: 0.9270\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0078 - acc: 0.9993 - val_loss: 0.2359 - val_acc: 0.9240\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100, total= 1.1min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 14s 5ms/step - loss: 0.7361 - acc: 0.6132 - val_loss: 0.5354 - val_acc: 0.7452\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.4543 - acc: 0.7866 - val_loss: 0.4164 - val_acc: 0.8227\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.3001 - acc: 0.8791 - val_loss: 0.3088 - val_acc: 0.8733\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2025 - acc: 0.9217 - val_loss: 0.2398 - val_acc: 0.9091\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1331 - acc: 0.9515 - val_loss: 0.1969 - val_acc: 0.9270\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0743 - acc: 0.9750 - val_loss: 0.1829 - val_acc: 0.9285\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0419 - acc: 0.9869 - val_loss: 0.1719 - val_acc: 0.9359\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0304 - acc: 0.9910 - val_loss: 0.1697 - val_acc: 0.9404\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0304 - acc: 0.9910 - val_loss: 0.2157 - val_acc: 0.9255\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0299 - acc: 0.9903 - val_loss: 0.2219 - val_acc: 0.9165\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0238 - acc: 0.9918 - val_loss: 0.1874 - val_acc: 0.9314\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0194 - acc: 0.9948 - val_loss: 0.1929 - val_acc: 0.9314\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0176 - acc: 0.9944 - val_loss: 0.1963 - val_acc: 0.9270\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0076 - acc: 0.9989 - val_loss: 0.1959 - val_acc: 0.9255\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0052 - acc: 0.9996 - val_loss: 0.1920 - val_acc: 0.9344\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 16s 6ms/step - loss: 1.0319 - acc: 0.5435 - val_loss: 0.6444 - val_acc: 0.6334\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5508 - acc: 0.7109 - val_loss: 0.5589 - val_acc: 0.7019\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.4274 - acc: 0.8101 - val_loss: 0.4300 - val_acc: 0.7988\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.3183 - acc: 0.8665 - val_loss: 0.3835 - val_acc: 0.8346\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1945 - acc: 0.9239 - val_loss: 0.3182 - val_acc: 0.8644\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1120 - acc: 0.9620 - val_loss: 0.2979 - val_acc: 0.8703\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1166 - acc: 0.9590 - val_loss: 0.2981 - val_acc: 0.8763\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0794 - acc: 0.9728 - val_loss: 0.2756 - val_acc: 0.8823\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0482 - acc: 0.9825 - val_loss: 0.2621 - val_acc: 0.8897\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0246 - acc: 0.9948 - val_loss: 0.2607 - val_acc: 0.8972\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0226 - acc: 0.9948 - val_loss: 0.2912 - val_acc: 0.8897\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0184 - acc: 0.9948 - val_loss: 0.2558 - val_acc: 0.9106\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0157 - acc: 0.9966 - val_loss: 0.2526 - val_acc: 0.9136\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0109 - acc: 0.9966 - val_loss: 0.2583 - val_acc: 0.9165\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0094 - acc: 0.9974 - val_loss: 0.3105 - val_acc: 0.8823\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150, total= 1.6min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 16s 6ms/step - loss: 0.9065 - acc: 0.5822 - val_loss: 0.5676 - val_acc: 0.6841\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5123 - acc: 0.7400 - val_loss: 0.4602 - val_acc: 0.7809\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.3775 - acc: 0.8336 - val_loss: 0.3832 - val_acc: 0.8241\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2329 - acc: 0.9082 - val_loss: 0.2831 - val_acc: 0.8778\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1394 - acc: 0.9496 - val_loss: 0.3058 - val_acc: 0.8510\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0894 - acc: 0.9739 - val_loss: 0.2552 - val_acc: 0.8972\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0494 - acc: 0.9862 - val_loss: 0.2611 - val_acc: 0.8957\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0351 - acc: 0.9881 - val_loss: 0.2690 - val_acc: 0.9046\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0353 - acc: 0.9888 - val_loss: 0.2494 - val_acc: 0.9121\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0247 - acc: 0.9933 - val_loss: 0.2554 - val_acc: 0.9061\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0192 - acc: 0.9944 - val_loss: 0.2977 - val_acc: 0.8778\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0241 - acc: 0.9933 - val_loss: 0.2690 - val_acc: 0.9061\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0207 - acc: 0.9948 - val_loss: 0.2505 - val_acc: 0.9031\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0192 - acc: 0.9948 - val_loss: 0.3320 - val_acc: 0.8957\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0083 - acc: 0.9985 - val_loss: 0.2554 - val_acc: 0.9136\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150, total= 1.6min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 16s 6ms/step - loss: 0.9498 - acc: 0.5729 - val_loss: 0.5879 - val_acc: 0.7377\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5206 - acc: 0.7464 - val_loss: 0.4893 - val_acc: 0.7750\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.4166 - acc: 0.8135 - val_loss: 0.4115 - val_acc: 0.8197\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.3299 - acc: 0.8639 - val_loss: 0.3317 - val_acc: 0.8793\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1999 - acc: 0.9284 - val_loss: 0.2433 - val_acc: 0.9180\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1128 - acc: 0.9612 - val_loss: 0.1972 - val_acc: 0.9225\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0672 - acc: 0.9795 - val_loss: 0.1913 - val_acc: 0.9240\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0443 - acc: 0.9877 - val_loss: 0.1829 - val_acc: 0.9270\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0332 - acc: 0.9903 - val_loss: 0.1816 - val_acc: 0.9463\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0323 - acc: 0.9903 - val_loss: 0.2098 - val_acc: 0.9195\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0213 - acc: 0.9952 - val_loss: 0.1829 - val_acc: 0.9359\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0174 - acc: 0.9963 - val_loss: 0.1955 - val_acc: 0.9300\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0150 - acc: 0.9966 - val_loss: 0.1977 - val_acc: 0.9359\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0151 - acc: 0.9948 - val_loss: 0.1914 - val_acc: 0.9374\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0070 - acc: 0.9989 - val_loss: 0.1926 - val_acc: 0.9419\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150, total= 1.6min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 14s 5ms/step - loss: 0.8985 - acc: 0.5412 - val_loss: 0.6656 - val_acc: 0.6587\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5484 - acc: 0.7374 - val_loss: 0.5089 - val_acc: 0.7198\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5149 - acc: 0.7393 - val_loss: 0.4079 - val_acc: 0.8152\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.4204 - acc: 0.8101 - val_loss: 0.3520 - val_acc: 0.8793\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2361 - acc: 0.9209 - val_loss: 0.3276 - val_acc: 0.8763\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1568 - acc: 0.9444 - val_loss: 0.3471 - val_acc: 0.8748\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1047 - acc: 0.9657 - val_loss: 0.2214 - val_acc: 0.9180\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0669 - acc: 0.9814 - val_loss: 0.3416 - val_acc: 0.8689\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1683 - acc: 0.9310 - val_loss: 0.2077 - val_acc: 0.9210\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0396 - acc: 0.9907 - val_loss: 0.1890 - val_acc: 0.9374\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0215 - acc: 0.9966 - val_loss: 0.2009 - val_acc: 0.9225\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0112 - acc: 0.9989 - val_loss: 0.2029 - val_acc: 0.9225\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.2093 - val_acc: 0.9240\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.2177 - val_acc: 0.9225\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.2315 - val_acc: 0.9195\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 15s 5ms/step - loss: 0.9617 - acc: 0.5767 - val_loss: 0.6685 - val_acc: 0.5768\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5883 - acc: 0.6751 - val_loss: 0.5391 - val_acc: 0.7452\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5031 - acc: 0.7475 - val_loss: 0.4971 - val_acc: 0.7258\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.3389 - acc: 0.8542 - val_loss: 0.6248 - val_acc: 0.7004\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5039 - acc: 0.7654 - val_loss: 0.3119 - val_acc: 0.8823\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2028 - acc: 0.9258 - val_loss: 0.3384 - val_acc: 0.8510\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1371 - acc: 0.9470 - val_loss: 0.2148 - val_acc: 0.9195\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0703 - acc: 0.9776 - val_loss: 0.2293 - val_acc: 0.9076\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2788 - acc: 0.8855 - val_loss: 0.2245 - val_acc: 0.9136\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0665 - acc: 0.9851 - val_loss: 0.2054 - val_acc: 0.9136\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0346 - acc: 0.9937 - val_loss: 0.2074 - val_acc: 0.9121\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0213 - acc: 0.9959 - val_loss: 0.2361 - val_acc: 0.9106\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0192 - acc: 0.9963 - val_loss: 0.2336 - val_acc: 0.9121\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0110 - acc: 0.9978 - val_loss: 0.2654 - val_acc: 0.9031\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0070 - acc: 0.9989 - val_loss: 0.2547 - val_acc: 0.9001\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 15s 6ms/step - loss: 0.8363 - acc: 0.5834 - val_loss: 0.6572 - val_acc: 0.5872\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5591 - acc: 0.7079 - val_loss: 0.4563 - val_acc: 0.7988\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.4729 - acc: 0.7773 - val_loss: 0.3682 - val_acc: 0.8763\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.3190 - acc: 0.8713 - val_loss: 0.2744 - val_acc: 0.9136\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1953 - acc: 0.9247 - val_loss: 0.2478 - val_acc: 0.9106\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1461 - acc: 0.9467 - val_loss: 1.2250 - val_acc: 0.5484\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5236 - acc: 0.7926 - val_loss: 0.1944 - val_acc: 0.9359\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1183 - acc: 0.9564 - val_loss: 0.1618 - val_acc: 0.9463\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0767 - acc: 0.9739 - val_loss: 0.1586 - val_acc: 0.9419\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0496 - acc: 0.9847 - val_loss: 0.1560 - val_acc: 0.9449\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0311 - acc: 0.9944 - val_loss: 0.1407 - val_acc: 0.9449\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0184 - acc: 0.9970 - val_loss: 0.1472 - val_acc: 0.9493\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0132 - acc: 0.9989 - val_loss: 0.1426 - val_acc: 0.9478\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0078 - acc: 0.9996 - val_loss: 0.1566 - val_acc: 0.9404\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0054 - acc: 0.9996 - val_loss: 0.1653 - val_acc: 0.9389\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100, total= 1.1min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 17s 6ms/step - loss: 1.1243 - acc: 0.5502 - val_loss: 0.5978 - val_acc: 0.6736\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5883 - acc: 0.6762 - val_loss: 0.5162 - val_acc: 0.7154\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5858 - acc: 0.6982 - val_loss: 0.6311 - val_acc: 0.6289\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.3876 - acc: 0.8191 - val_loss: 0.3852 - val_acc: 0.8450\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2371 - acc: 0.9112 - val_loss: 0.8048 - val_acc: 0.5887\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.8532 - acc: 0.6404 - val_loss: 0.3110 - val_acc: 0.8584\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2132 - acc: 0.9187 - val_loss: 0.2364 - val_acc: 0.9151\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1606 - acc: 0.9377 - val_loss: 0.2160 - val_acc: 0.9270\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0970 - acc: 0.9728 - val_loss: 0.1997 - val_acc: 0.9240\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0671 - acc: 0.9810 - val_loss: 0.2319 - val_acc: 0.9210\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0471 - acc: 0.9888 - val_loss: 0.2022 - val_acc: 0.9270\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0291 - acc: 0.9925 - val_loss: 0.2019 - val_acc: 0.9270\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0145 - acc: 0.9966 - val_loss: 0.2071 - val_acc: 0.9210\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0083 - acc: 0.9993 - val_loss: 0.2229 - val_acc: 0.9240\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0052 - acc: 0.9996 - val_loss: 0.2276 - val_acc: 0.9210\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150, total= 1.7min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 17s 6ms/step - loss: 1.2138 - acc: 0.5658 - val_loss: 0.6044 - val_acc: 0.6900\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.6040 - acc: 0.6695 - val_loss: 0.5983 - val_acc: 0.6393\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5198 - acc: 0.7341 - val_loss: 0.8785 - val_acc: 0.4262\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.6394 - acc: 0.6639 - val_loss: 0.4247 - val_acc: 0.8077\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.4548 - acc: 0.7930 - val_loss: 0.4308 - val_acc: 0.8301\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2487 - acc: 0.9161 - val_loss: 0.2919 - val_acc: 0.8808\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1700 - acc: 0.9403 - val_loss: 0.4735 - val_acc: 0.7690\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1502 - acc: 0.9474 - val_loss: 0.2264 - val_acc: 0.9180\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0651 - acc: 0.9862 - val_loss: 0.2028 - val_acc: 0.9270\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0347 - acc: 0.9925 - val_loss: 0.1956 - val_acc: 0.9270\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0178 - acc: 0.9978 - val_loss: 0.2116 - val_acc: 0.9255\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0096 - acc: 0.9989 - val_loss: 0.2263 - val_acc: 0.9195\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0059 - acc: 0.9993 - val_loss: 0.2162 - val_acc: 0.9314\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0039 - acc: 0.9996 - val_loss: 0.2310 - val_acc: 0.9300\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0035 - acc: 0.9993 - val_loss: 0.2413 - val_acc: 0.9225\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150, total= 1.7min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 18s 7ms/step - loss: 1.1448 - acc: 0.5390 - val_loss: 0.6239 - val_acc: 0.6915\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5758 - acc: 0.6826 - val_loss: 0.4753 - val_acc: 0.8092\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.6086 - acc: 0.7113 - val_loss: 0.4704 - val_acc: 0.8003\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.4566 - acc: 0.7866 - val_loss: 0.3352 - val_acc: 0.8793\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.3229 - acc: 0.8627 - val_loss: 0.2817 - val_acc: 0.9195\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1607 - acc: 0.9564 - val_loss: 0.2152 - val_acc: 0.9240\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1004 - acc: 0.9702 - val_loss: 0.2283 - val_acc: 0.9061\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0937 - acc: 0.9687 - val_loss: 0.1615 - val_acc: 0.9493\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0406 - acc: 0.9922 - val_loss: 0.1490 - val_acc: 0.9523\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0211 - acc: 0.9966 - val_loss: 0.1650 - val_acc: 0.9419\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0127 - acc: 0.9989 - val_loss: 0.1490 - val_acc: 0.9478\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0066 - acc: 0.9993 - val_loss: 0.1587 - val_acc: 0.9434\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0048 - acc: 0.9996 - val_loss: 0.1703 - val_acc: 0.9389\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0035 - acc: 0.9996 - val_loss: 0.1746 - val_acc: 0.9419\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0026 - acc: 0.9996 - val_loss: 0.1722 - val_acc: 0.9374\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.3, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150, total= 1.7min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 17s 6ms/step - loss: 0.9149 - acc: 0.5677 - val_loss: 0.6573 - val_acc: 0.6215\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5351 - acc: 0.7266 - val_loss: 0.5498 - val_acc: 0.7347\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.4050 - acc: 0.8169 - val_loss: 0.4737 - val_acc: 0.8122\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2771 - acc: 0.8847 - val_loss: 0.3511 - val_acc: 0.8644\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1799 - acc: 0.9325 - val_loss: 0.3123 - val_acc: 0.8838\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0994 - acc: 0.9646 - val_loss: 0.2698 - val_acc: 0.8897\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0628 - acc: 0.9806 - val_loss: 0.2527 - val_acc: 0.8942\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0394 - acc: 0.9884 - val_loss: 0.2417 - val_acc: 0.8987\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0345 - acc: 0.9881 - val_loss: 0.2522 - val_acc: 0.8987\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0294 - acc: 0.9929 - val_loss: 0.2551 - val_acc: 0.8942\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0225 - acc: 0.9948 - val_loss: 0.2942 - val_acc: 0.8823\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0247 - acc: 0.9929 - val_loss: 0.2523 - val_acc: 0.9001\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0268 - acc: 0.9918 - val_loss: 0.2557 - val_acc: 0.9001\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0110 - acc: 0.9981 - val_loss: 0.2614 - val_acc: 0.9031\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0071 - acc: 0.9989 - val_loss: 0.2465 - val_acc: 0.9031\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100, total= 1.3min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 17s 6ms/step - loss: 0.9509 - acc: 0.5744 - val_loss: 0.6028 - val_acc: 0.7198\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5314 - acc: 0.7367 - val_loss: 0.5218 - val_acc: 0.7511\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.3877 - acc: 0.8288 - val_loss: 0.4410 - val_acc: 0.7958\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2598 - acc: 0.8956 - val_loss: 0.3555 - val_acc: 0.8703\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1882 - acc: 0.9265 - val_loss: 0.3134 - val_acc: 0.8748\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1109 - acc: 0.9601 - val_loss: 0.2903 - val_acc: 0.8838\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0664 - acc: 0.9776 - val_loss: 0.2749 - val_acc: 0.9031\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0440 - acc: 0.9873 - val_loss: 0.2416 - val_acc: 0.9001\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0282 - acc: 0.9925 - val_loss: 0.3066 - val_acc: 0.8852\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0243 - acc: 0.9940 - val_loss: 0.2473 - val_acc: 0.8897\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0279 - acc: 0.9907 - val_loss: 0.2602 - val_acc: 0.8912\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0565 - acc: 0.9806 - val_loss: 0.2243 - val_acc: 0.9001\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0466 - acc: 0.9851 - val_loss: 0.2193 - val_acc: 0.9091\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0241 - acc: 0.9937 - val_loss: 0.2169 - val_acc: 0.9091\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0379 - acc: 0.9847 - val_loss: 0.2220 - val_acc: 0.9091\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 17s 6ms/step - loss: 0.9036 - acc: 0.5912 - val_loss: 0.6091 - val_acc: 0.6513\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5222 - acc: 0.7449 - val_loss: 0.5188 - val_acc: 0.7794\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.4047 - acc: 0.8277 - val_loss: 0.4442 - val_acc: 0.8077\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2964 - acc: 0.8806 - val_loss: 0.3852 - val_acc: 0.8286\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2021 - acc: 0.9191 - val_loss: 0.2782 - val_acc: 0.9061\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1287 - acc: 0.9541 - val_loss: 0.2345 - val_acc: 0.9240\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0830 - acc: 0.9683 - val_loss: 0.2227 - val_acc: 0.9270\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0690 - acc: 0.9743 - val_loss: 0.2110 - val_acc: 0.9329\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0592 - acc: 0.9814 - val_loss: 0.2282 - val_acc: 0.9180\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0421 - acc: 0.9851 - val_loss: 0.1959 - val_acc: 0.9329\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0324 - acc: 0.9903 - val_loss: 0.2031 - val_acc: 0.9300\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0206 - acc: 0.9952 - val_loss: 0.1927 - val_acc: 0.9329\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0135 - acc: 0.9970 - val_loss: 0.1946 - val_acc: 0.9300\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0097 - acc: 0.9981 - val_loss: 0.1835 - val_acc: 0.9389\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0064 - acc: 0.9989 - val_loss: 0.1868 - val_acc: 0.9389\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 19s 7ms/step - loss: 1.2096 - acc: 0.5520 - val_loss: 0.6115 - val_acc: 0.7094\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5494 - acc: 0.7135 - val_loss: 0.5311 - val_acc: 0.7645\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.4318 - acc: 0.7993 - val_loss: 0.4641 - val_acc: 0.7884\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.3278 - acc: 0.8695 - val_loss: 0.4175 - val_acc: 0.8063\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2261 - acc: 0.9135 - val_loss: 0.3906 - val_acc: 0.8450\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1386 - acc: 0.9504 - val_loss: 0.2965 - val_acc: 0.8703\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0831 - acc: 0.9728 - val_loss: 0.2959 - val_acc: 0.8778\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0672 - acc: 0.9765 - val_loss: 0.2762 - val_acc: 0.8808\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0514 - acc: 0.9832 - val_loss: 0.2857 - val_acc: 0.8912\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0402 - acc: 0.9884 - val_loss: 0.2605 - val_acc: 0.8912\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0222 - acc: 0.9944 - val_loss: 0.3216 - val_acc: 0.8808\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0190 - acc: 0.9940 - val_loss: 0.2980 - val_acc: 0.8823\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0353 - acc: 0.9888 - val_loss: 0.2571 - val_acc: 0.9016\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0365 - acc: 0.9866 - val_loss: 0.5059 - val_acc: 0.7675\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.3298 - acc: 0.8799 - val_loss: 0.3463 - val_acc: 0.8510\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150, total= 1.7min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 20s 7ms/step - loss: 1.2296 - acc: 0.5673 - val_loss: 0.6505 - val_acc: 0.6334\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5561 - acc: 0.7102 - val_loss: 0.5224 - val_acc: 0.7481\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.4413 - acc: 0.7937 - val_loss: 0.4984 - val_acc: 0.7735\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.3008 - acc: 0.8758 - val_loss: 0.4093 - val_acc: 0.8405\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2453 - acc: 0.9064 - val_loss: 0.3339 - val_acc: 0.8703\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1691 - acc: 0.9340 - val_loss: 0.2817 - val_acc: 0.9031\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1019 - acc: 0.9627 - val_loss: 0.2352 - val_acc: 0.9061\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0612 - acc: 0.9821 - val_loss: 0.2572 - val_acc: 0.8957\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0408 - acc: 0.9877 - val_loss: 0.2355 - val_acc: 0.9061\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0233 - acc: 0.9955 - val_loss: 0.2324 - val_acc: 0.9061\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0150 - acc: 0.9966 - val_loss: 0.2567 - val_acc: 0.9001\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0111 - acc: 0.9981 - val_loss: 0.2585 - val_acc: 0.9016\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0101 - acc: 0.9989 - val_loss: 0.2916 - val_acc: 0.9061\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0335 - acc: 0.9892 - val_loss: 0.2575 - val_acc: 0.8942\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0597 - acc: 0.9780 - val_loss: 0.2630 - val_acc: 0.8912\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150, total= 1.8min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 20s 8ms/step - loss: 1.1774 - acc: 0.5401 - val_loss: 0.6556 - val_acc: 0.6244\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5890 - acc: 0.6960 - val_loss: 0.5350 - val_acc: 0.7675\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.4561 - acc: 0.7949 - val_loss: 0.4545 - val_acc: 0.8316\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.3436 - acc: 0.8586 - val_loss: 0.3819 - val_acc: 0.8539\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2370 - acc: 0.9056 - val_loss: 0.2957 - val_acc: 0.8972\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1532 - acc: 0.9392 - val_loss: 0.2613 - val_acc: 0.8957\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0961 - acc: 0.9668 - val_loss: 0.3029 - val_acc: 0.9121\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2343 - acc: 0.9138 - val_loss: 0.2994 - val_acc: 0.8763\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2248 - acc: 0.9056 - val_loss: 0.3134 - val_acc: 0.8927\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1902 - acc: 0.9265 - val_loss: 0.2894 - val_acc: 0.9091\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1400 - acc: 0.9485 - val_loss: 0.2128 - val_acc: 0.9136\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0882 - acc: 0.9709 - val_loss: 0.1849 - val_acc: 0.9285\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0625 - acc: 0.9799 - val_loss: 0.1775 - val_acc: 0.9314\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0369 - acc: 0.9881 - val_loss: 0.2228 - val_acc: 0.9180\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0259 - acc: 0.9922 - val_loss: 0.1637 - val_acc: 0.9389\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Adam'>, units=150, total= 1.7min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 18s 7ms/step - loss: 1.0970 - acc: 0.5718 - val_loss: 0.5897 - val_acc: 0.6990\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5524 - acc: 0.7128 - val_loss: 0.6182 - val_acc: 0.6692\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5331 - acc: 0.7277 - val_loss: 0.5909 - val_acc: 0.6572\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.4663 - acc: 0.7728 - val_loss: 0.4685 - val_acc: 0.7735\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2928 - acc: 0.8821 - val_loss: 0.3417 - val_acc: 0.8763\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.3079 - acc: 0.8698 - val_loss: 0.3187 - val_acc: 0.8882\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1286 - acc: 0.9612 - val_loss: 0.2657 - val_acc: 0.9046\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2209 - acc: 0.9030 - val_loss: 0.2956 - val_acc: 0.9031\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0920 - acc: 0.9772 - val_loss: 0.2503 - val_acc: 0.8972\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0489 - acc: 0.9888 - val_loss: 0.2605 - val_acc: 0.9031\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0285 - acc: 0.9959 - val_loss: 0.2410 - val_acc: 0.9091\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0157 - acc: 0.9981 - val_loss: 0.2462 - val_acc: 0.9106\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0092 - acc: 0.9993 - val_loss: 0.2443 - val_acc: 0.9106\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0080 - acc: 0.9993 - val_loss: 0.2463 - val_acc: 0.9136\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0045 - acc: 0.9996 - val_loss: 0.2518 - val_acc: 0.9076\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 19s 7ms/step - loss: 1.0601 - acc: 0.5423 - val_loss: 0.6293 - val_acc: 0.6706\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.6117 - acc: 0.6796 - val_loss: 0.5574 - val_acc: 0.7481\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5270 - acc: 0.7288 - val_loss: 0.5203 - val_acc: 0.7809\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.4551 - acc: 0.7866 - val_loss: 0.4231 - val_acc: 0.8271\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.3652 - acc: 0.8471 - val_loss: 0.5378 - val_acc: 0.7362\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2722 - acc: 0.8918 - val_loss: 0.3082 - val_acc: 0.8852\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1750 - acc: 0.9388 - val_loss: 0.3490 - val_acc: 0.8703\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1331 - acc: 0.9504 - val_loss: 0.2536 - val_acc: 0.9091\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0688 - acc: 0.9832 - val_loss: 0.2268 - val_acc: 0.9255\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0395 - acc: 0.9914 - val_loss: 0.2318 - val_acc: 0.9180\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0222 - acc: 0.9974 - val_loss: 0.2105 - val_acc: 0.9225\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0145 - acc: 0.9974 - val_loss: 0.3960 - val_acc: 0.8480\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 1.9147 - acc: 0.5692 - val_loss: 0.7985 - val_acc: 0.3994\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5625 - acc: 0.7057 - val_loss: 0.4917 - val_acc: 0.8107\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.4729 - acc: 0.7676 - val_loss: 0.4062 - val_acc: 0.8897\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 19s 7ms/step - loss: 1.0601 - acc: 0.5587 - val_loss: 0.6468 - val_acc: 0.6304\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.6092 - acc: 0.6512 - val_loss: 0.5389 - val_acc: 0.7928\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5060 - acc: 0.7553 - val_loss: 0.5129 - val_acc: 0.7124\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.4626 - acc: 0.7773 - val_loss: 0.4617 - val_acc: 0.7928\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.2920 - acc: 0.8862 - val_loss: 0.2886 - val_acc: 0.9180\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1989 - acc: 0.9198 - val_loss: 0.2407 - val_acc: 0.9344\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1217 - acc: 0.9567 - val_loss: 0.2749 - val_acc: 0.8808\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.5955 - acc: 0.7755 - val_loss: 0.2474 - val_acc: 0.9180\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.1390 - acc: 0.9530 - val_loss: 0.1928 - val_acc: 0.9419\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0953 - acc: 0.9657 - val_loss: 0.1864 - val_acc: 0.9314\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0623 - acc: 0.9802 - val_loss: 0.1814 - val_acc: 0.9195\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0435 - acc: 0.9877 - val_loss: 0.1592 - val_acc: 0.9463\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0297 - acc: 0.9929 - val_loss: 0.1603 - val_acc: 0.9463\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0184 - acc: 0.9974 - val_loss: 0.1527 - val_acc: 0.9449\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 3s 1ms/step - loss: 0.0115 - acc: 0.9985 - val_loss: 0.1550 - val_acc: 0.9449\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=100, total= 1.2min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 21s 8ms/step - loss: 1.4536 - acc: 0.5334 - val_loss: 0.6589 - val_acc: 0.6215\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5697 - acc: 0.7296 - val_loss: 0.9799 - val_acc: 0.3875\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.7038 - acc: 0.6632 - val_loss: 0.4994 - val_acc: 0.7720\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.4379 - acc: 0.7963 - val_loss: 0.4860 - val_acc: 0.8048\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.3598 - acc: 0.8419 - val_loss: 0.4582 - val_acc: 0.8152\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.3561 - acc: 0.8445 - val_loss: 0.3524 - val_acc: 0.8718\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1573 - acc: 0.9500 - val_loss: 0.4563 - val_acc: 0.7973\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2233 - acc: 0.9079 - val_loss: 0.2746 - val_acc: 0.9076\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0807 - acc: 0.9772 - val_loss: 0.2726 - val_acc: 0.8897\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0428 - acc: 0.9910 - val_loss: 0.2366 - val_acc: 0.9165\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0205 - acc: 0.9970 - val_loss: 0.2445 - val_acc: 0.9076\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0203 - acc: 0.9970 - val_loss: 0.2498 - val_acc: 0.9076\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 1.2040 - acc: 0.8486 - val_loss: 0.5654 - val_acc: 0.8376\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 1.2405 - acc: 0.6576 - val_loss: 0.3558 - val_acc: 0.8897\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2186 - acc: 0.9228 - val_loss: 0.2591 - val_acc: 0.9225\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150, total= 1.8min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 22s 8ms/step - loss: 1.5600 - acc: 0.5431 - val_loss: 0.6549 - val_acc: 0.6468\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.7370 - acc: 0.6419 - val_loss: 0.6494 - val_acc: 0.6259\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5602 - acc: 0.7180 - val_loss: 0.5525 - val_acc: 0.7660\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.4959 - acc: 0.7740 - val_loss: 0.5417 - val_acc: 0.6870\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.4203 - acc: 0.7986 - val_loss: 0.4513 - val_acc: 0.8361\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2515 - acc: 0.9071 - val_loss: 0.3467 - val_acc: 0.8569\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.4052 - acc: 0.8318 - val_loss: 0.4224 - val_acc: 0.8182\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2090 - acc: 0.9220 - val_loss: 0.3361 - val_acc: 0.8435\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1302 - acc: 0.9593 - val_loss: 0.3337 - val_acc: 0.8659\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0892 - acc: 0.9717 - val_loss: 0.3752 - val_acc: 0.8361\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0562 - acc: 0.9851 - val_loss: 0.2596 - val_acc: 0.8972\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0264 - acc: 0.9955 - val_loss: 0.2287 - val_acc: 0.9121\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0130 - acc: 0.9985 - val_loss: 0.2233 - val_acc: 0.9121\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0070 - acc: 0.9993 - val_loss: 0.2244 - val_acc: 0.9165\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0048 - acc: 0.9993 - val_loss: 0.2252 - val_acc: 0.9165\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150, total= 1.7min\n",
            "[CV] activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150 \n",
            "Train on 2681 samples, validate on 671 samples\n",
            "Epoch 1/15\n",
            "2681/2681 [==============================] - 22s 8ms/step - loss: 1.5340 - acc: 0.5390 - val_loss: 0.6034 - val_acc: 0.7362\n",
            "Epoch 2/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5848 - acc: 0.6833 - val_loss: 0.6365 - val_acc: 0.6200\n",
            "Epoch 3/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.5335 - acc: 0.7404 - val_loss: 0.4518 - val_acc: 0.8361\n",
            "Epoch 4/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.4801 - acc: 0.7710 - val_loss: 0.4183 - val_acc: 0.8763\n",
            "Epoch 5/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.3775 - acc: 0.8325 - val_loss: 0.3389 - val_acc: 0.9016\n",
            "Epoch 6/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.3699 - acc: 0.8333 - val_loss: 0.3225 - val_acc: 0.9151\n",
            "Epoch 7/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1708 - acc: 0.9396 - val_loss: 0.2404 - val_acc: 0.9314\n",
            "Epoch 8/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.2782 - acc: 0.8870 - val_loss: 0.2915 - val_acc: 0.9180\n",
            "Epoch 9/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.1221 - acc: 0.9601 - val_loss: 0.2323 - val_acc: 0.9314\n",
            "Epoch 10/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0710 - acc: 0.9799 - val_loss: 0.1898 - val_acc: 0.9493\n",
            "Epoch 11/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0415 - acc: 0.9910 - val_loss: 0.1736 - val_acc: 0.9493\n",
            "Epoch 12/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0233 - acc: 0.9963 - val_loss: 0.1622 - val_acc: 0.9493\n",
            "Epoch 13/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0146 - acc: 0.9978 - val_loss: 0.1623 - val_acc: 0.9419\n",
            "Epoch 14/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0091 - acc: 0.9989 - val_loss: 0.1576 - val_acc: 0.9419\n",
            "Epoch 15/15\n",
            "2681/2681 [==============================] - 5s 2ms/step - loss: 0.0056 - acc: 0.9996 - val_loss: 0.1593 - val_acc: 0.9419\n",
            "[CV]  activation=sigmoid, batch_size=256, dropout=0.5, epochs=15, loss=binary_crossentropy, optimizer=<class 'keras.optimizers.Nadam'>, units=150, total= 1.7min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed: 69.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 4022 samples, validate on 1006 samples\n",
            "Epoch 1/15\n",
            "4022/4022 [==============================] - 23s 6ms/step - loss: 0.6289 - acc: 0.6907 - val_loss: 0.5278 - val_acc: 0.7565\n",
            "Epoch 2/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.3173 - acc: 0.8707 - val_loss: 0.3468 - val_acc: 0.8539\n",
            "Epoch 3/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.1712 - acc: 0.9339 - val_loss: 0.2637 - val_acc: 0.9085\n",
            "Epoch 4/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.1009 - acc: 0.9615 - val_loss: 0.2605 - val_acc: 0.8966\n",
            "Epoch 5/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.0915 - acc: 0.9632 - val_loss: 0.2089 - val_acc: 0.9235\n",
            "Epoch 6/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.0745 - acc: 0.9734 - val_loss: 0.2097 - val_acc: 0.9304\n",
            "Epoch 7/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.0879 - acc: 0.9644 - val_loss: 0.2270 - val_acc: 0.9185\n",
            "Epoch 8/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.0636 - acc: 0.9756 - val_loss: 0.2064 - val_acc: 0.9205\n",
            "Epoch 9/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.0399 - acc: 0.9848 - val_loss: 0.2368 - val_acc: 0.9145\n",
            "Epoch 10/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.0265 - acc: 0.9913 - val_loss: 0.1877 - val_acc: 0.9304\n",
            "Epoch 11/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.0201 - acc: 0.9920 - val_loss: 0.1751 - val_acc: 0.9404\n",
            "Epoch 12/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.0163 - acc: 0.9938 - val_loss: 0.1890 - val_acc: 0.9364\n",
            "Epoch 13/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.0080 - acc: 0.9983 - val_loss: 0.1928 - val_acc: 0.9374\n",
            "Epoch 14/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.0054 - acc: 0.9988 - val_loss: 0.2013 - val_acc: 0.9324\n",
            "Epoch 15/15\n",
            "4022/4022 [==============================] - 6s 1ms/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.2077 - val_acc: 0.9264\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBX3V48rJEfl",
        "colab_type": "code",
        "outputId": "edce2ee2-0d39-46f6-c892-efff228f84ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dump(tokenizer, datapath + 'sentiment_tok.joblib.gz', compress='gzip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/My Drive/Colab Notebooks/data/sentiment_tok.joblib.gz']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1dwVv4tUlb1",
        "colab_type": "code",
        "outputId": "9d76f1aa-48cd-416f-9a73-6e7aa84c8c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "embedding_matrix = load(datapath + 'embedding_matrix.joblib.gz')\n",
        "EMBEDDING_DIM = embedding_matrix.shape[1]\n",
        "MAX_VOCAB_SIZE = embedding_matrix.shape[0]\n",
        "print('Found %s by %s word vectors.' % (MAX_VOCAB_SIZE, EMBEDDING_DIM))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 11205 by 100 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "In-AxTY1k2Pw",
        "colab_type": "code",
        "outputId": "663bec53-c1f2-4c83-b088-66cd50c0e3b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "MAX_VOCAB_SIZE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11205"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOgEm6vTlKqJ",
        "colab_type": "code",
        "outputId": "17732aa6-a4dd-4c37-afea-6f6afde01a93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11204, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv3OHcn-lO75",
        "colab_type": "code",
        "outputId": "1957eb96-a7c5-4924-b340-572eea04fa2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(word_vectors.vector_size)\n",
        "print(len(word_vectors.wv.index2word) + 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "11204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kjcNW_dlwRS",
        "colab_type": "code",
        "outputId": "7b2a9013-9b13-4488-abaa-da2c2d7c7099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dump(embedding_matrix, datapath + 'embedding_matrix.joblib.gz', compress='gzip')\n",
        "dump(tokenizer, datapath + 'tokenizer.joblib.gz', compress='gzip')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/gdrive/My Drive/Colab Notebooks/data/tokenizer.joblib.gz']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ap5L5VfkEh1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}